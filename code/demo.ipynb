{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"PySpark Script: \", sys.argv[0])\n",
    "\n",
    "# Create a spark context and print some information about the context object\n",
    "spark: SparkSession = SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.version)\n",
    "print(spark.sparkContext.pythonVer)\n",
    "print(spark.sparkContext.master)\n",
    "# Stop Pyspark\n",
    "spark.stop()\n",
    "print(\"Spark Successfully Stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "DEFAULT_DATA_SIZE = 10000000\n",
    "\n",
    "data_size_labels = {\n",
    "    50000000: 'large',\n",
    "    10000000: 'small'\n",
    "}\n",
    "\n",
    "def generate_data(size):\n",
    "    return [(random.randint(1, 10000), random.random()) for _ in range(size)]\n",
    "\n",
    "\n",
    "def run_large_spark_job(spark: SparkSession, data_size: int, app_name: str, generate_rdd: bool = False):\n",
    "    '''\n",
    "    Runs the spark job with the passed in SparkSession that contains an optimized\n",
    "    SparkConf\n",
    "    '''\n",
    "    print(f\"Running spark job for {app_name}\")\n",
    "    # conf = spark.sparkContext.getConf()\n",
    "    # for key, value in conf.getAll():\n",
    "    #     print(f\"{key} = {value}\")\n",
    "    # print()\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        sc = spark.sparkContext\n",
    "        sc.setLogLevel(\"ERROR\")\n",
    "        # Create a large DataFrame with random data\n",
    "        data = [(i, i % 100, i % 1000) for i in range(data_size)]\n",
    "        columns = [\"id\", \"group\", \"subgroup\"]\n",
    "        large_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "        # Perform transformation, aggregation, and sorting: shuffling and sorting\n",
    "        result_df = (large_df\n",
    "                    .withColumn(\"group_id\", F.col(\"group\") * 10)\n",
    "                    .groupBy(\"group_id\")\n",
    "                    .agg(F.avg(\"subgroup\").alias(\"avg_subgroup\"),\n",
    "                        F.count(\"id\").alias(\"count\"))\n",
    "                    .orderBy(\"avg_subgroup\"))\n",
    "        result_df.show()\n",
    "        # # Create small dataframe & join operation which will trigger a shuffle\n",
    "        # small_df = spark.createDataFrame([(i, i * 2) for i in range(100)], [\"group_id\", \"value\"])\n",
    "        # # Perform the join operation\n",
    "        # joined_df = result_df.join(small_df, \"group_id\")\n",
    "        # joined_df.show()\n",
    "        end_time = time.time()\n",
    "        print(\"Execution Time (secs):\", (end_time-start_time))\n",
    "    except Exception as e:\n",
    "        print('Stopping context with error', e)\n",
    "        spark.stop()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Default Spark Config\n",
    "\n",
    "- `spark.serializer` = org.apache.spark.serializer.JavaSerializer\n",
    "\n",
    "- `spark.reducer.maxSizeInFlight` = 48m\n",
    "\n",
    "- `spark.shuffle.compress` = true\n",
    "\n",
    "- `spark.shuffle.spill.compress` = true\n",
    "\n",
    "- `spark.rdd.compress` = true\n",
    "\n",
    "- `spark.shuffle.file.buffer` = 32k\n",
    "\n",
    "- `spark.shuffle.io.preferDirectBufs` = true\n",
    "\n",
    "- `spark.io.compression.codec` = lz4\n",
    "\n",
    "\n",
    "\n",
    "Source: https://spark.apache.org/docs/latest/tuning.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for Baseline: Default Config (data_size=small)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 99.55303621292114\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Baseline: Default Config (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Changing the serializer\n",
    "\n",
    "spark.serializer = JavaSerializer -> KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for Experiment 1: KryoSerializer (data_size=small)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 99.18301320075989\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 1: KryoSerializer (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2.1: Decreasing the Shuffle File Buffer\n",
    "- spark.shuffle.file.buffer = 32k -> 16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for Experiment 2.1: Decrease file_buffer to 16k (data_size=small)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 97.55631804466248\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 2.1: Decrease file_buffer to 16k (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"16k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2.2: Increasing the Shuffle File Buffer\n",
    "- spark.shuffle.file.buffer = 32k -> 64k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for Experiment 2.2: Increase file buffer to 64k (data_size=small)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 99.2040319442749\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 2.2: Increase file buffer to 64k (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"64k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3.1: Turn off shuffle compress\n",
    "- spark.shuffle.compress = true -> false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for Experiment 3.1: No Shuffle Compression (data_size=small)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 98.6578140258789\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 3.1: No Shuffle Compression (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"false\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3.2: Turn off shuffle.spill compress\n",
    "- spark.shuffle.spill.compress = true -> false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for Experiment 3.2: No Shuffle Spill Compression (data_size=small)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 97.03834915161133\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 3.2: No Shuffle Spill Compression (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"false\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4.1: Change compression codec to: zstd\n",
    "- spark.io.compression.codec = lz4 -> zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for Experiment 4.1: compression=zstd (data_size=small)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 100.85207486152649\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 4.1: compression=zstd (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"zstd\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4.2: Change compression codec to: snappy\n",
    "- spark.io.compression.codec = lz4 -> snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for Experiment 4.2: compression=snappy (data_size=small)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 103.56076312065125\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 4.2: compression=snappy (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4.3: Change compression codec to: lzf\n",
    "- spark.io.compression.codec = lz4 -> lzf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for Experiment 4.3: compression=lzf (data_size=small)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 100.27239227294922\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 4.3: compression=lzf (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lzf\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Optimization\n",
    "\n",
    "TODO: Figure out the final optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the job with default config\n",
    "# data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "# app_name = f'Final Optimization (data_size={data_size_label})'\n",
    "# spark: SparkSession = SparkSession.builder \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .appName(app_name) \\\n",
    "#     .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "#     .config(\"spark.shuffle.compress\", \"false\") \\\n",
    "#     .config(\"spark.io.compression.codec\", \"zstd\") \\\n",
    "#     .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "#     .config(\"spark.serializer\", \"org.apache.spark.serializer.KyroSerializer\") \\\n",
    "#     .config(\"spark.shuffle.file.buffer\", \"64k\") \\\n",
    "#     .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "#     .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "#     .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "#     .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "#     .getOrCreate()\n",
    "# run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (METCS777 Term Paper)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
