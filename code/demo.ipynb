{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"PySpark Script: \", sys.argv[0])\n",
    "\n",
    "# Create a spark context and print some information about the context object\n",
    "spark: SparkSession = SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.version)\n",
    "print(spark.sparkContext.pythonVer)\n",
    "print(spark.sparkContext.master)\n",
    "# Stop Pyspark\n",
    "spark.stop()\n",
    "print(\"Spark Successfully Stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "DEFAULT_DATA_SIZE = 10000000\n",
    "\n",
    "data_size_labels = {\n",
    "    50000000: 'large',\n",
    "    10000000: 'small'\n",
    "}\n",
    "\n",
    "def generate_data(size):\n",
    "    return [(random.randint(1, 10000), random.random()) for _ in range(size)]\n",
    "\n",
    "\n",
    "def run_large_spark_job(spark: SparkSession, data_size: int, app_name: str, generate_rdd: bool = False):\n",
    "    '''\n",
    "    Runs the spark job with the passed in SparkSession that contains an optimized\n",
    "    SparkConf\n",
    "    '''\n",
    "    print(\"Running spark job for SparkConf:\")\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    for key, value in conf.getAll():\n",
    "        print(f\"{key} = {value}\")\n",
    "    print()\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        sc = spark.sparkContext\n",
    "        sc.setLogLevel(\"ERROR\")\n",
    "        # Create a large DataFrame with random data\n",
    "        data = [(i, i % 100, i % 1000) for i in range(data_size)]\n",
    "        columns = [\"id\", \"group\", \"subgroup\"]\n",
    "        large_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "        # Perform transformation, aggregation, and sorting: shuffling and sorting\n",
    "        result_df = (large_df\n",
    "                    .withColumn(\"group_id\", F.col(\"group\") * 10)\n",
    "                    .groupBy(\"group_id\")\n",
    "                    .agg(F.avg(\"subgroup\").alias(\"avg_subgroup\"),\n",
    "                        F.count(\"id\").alias(\"count\"))\n",
    "                    .orderBy(\"avg_subgroup\"))\n",
    "        result_df.show()\n",
    "        # # Create small dataframe & join operation which will trigger a shuffle\n",
    "        # small_df = spark.createDataFrame([(i, i * 2) for i in range(100)], [\"group_id\", \"value\"])\n",
    "        # # Perform the join operation\n",
    "        # joined_df = result_df.join(small_df, \"group_id\")\n",
    "        # joined_df.show()\n",
    "        end_time = time.time()\n",
    "        print(\"Execution Time (secs):\", (end_time-start_time))\n",
    "    except Exception as e:\n",
    "        print('Stopping context with error', e)\n",
    "        spark.stop()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Default Spark Config\n",
    "\n",
    "- `spark.serializer` = org.apache.spark.serializer.JavaSerializer\n",
    "\n",
    "- `spark.reducer.maxSizeInFlight` = 48m\n",
    "\n",
    "- `spark.shuffle.compress` = true\n",
    "\n",
    "- `spark.shuffle.spill.compress` = true\n",
    "\n",
    "- `spark.rdd.compress` = true\n",
    "\n",
    "- `spark.shuffle.file.buffer` = 32k\n",
    "\n",
    "- `spark.shuffle.io.preferDirectBufs` = true\n",
    "\n",
    "- `spark.io.compression.codec` = lz4\n",
    "\n",
    "\n",
    "\n",
    "Source: https://spark.apache.org/docs/latest/tuning.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/20 17:39:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for SparkConf:\n",
      "spark.eventLog.enabled = true\n",
      "spark.reducer.maxSizeInFlight = 48m\n",
      "spark.serializer = org.apache.spark.serializer.JavaSerializer\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.app.id = local-1710970751255\n",
      "spark.shuffle.compress = true\n",
      "spark.executor.id = driver\n",
      "spark.shuffle.spill.compress = true\n",
      "spark.shuffle.file.buffer = 32k\n",
      "spark.eventLog.dir = /tmp/spark-events\n",
      "spark.driver.port = 63355\n",
      "spark.io.compression.codec = lz4\n",
      "spark.rdd.compress = True\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.app.startTime = 1710970750774\n",
      "spark.shuffle.io.preferDirectBufs = true\n",
      "spark.master = local[*]\n",
      "spark.submit.pyFiles = \n",
      "spark.app.submitTime = 1710970750605\n",
      "spark.submit.deployMode = client\n",
      "spark.driver.host = alexandrias-mbp\n",
      "spark.eventLog.compress = true\n",
      "spark.ui.showConsoleProgress = true\n",
      "spark.app.name = Baseline: Default Config (data_size=small)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 100.58739495277405\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Baseline: Default Config (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Changing the serializer\n",
    "\n",
    "spark.serializer = JavaSerializer -> KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for SparkConf:\n",
      "spark.eventLog.enabled = true\n",
      "spark.reducer.maxSizeInFlight = 48m\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.app.id = local-1710970891025\n",
      "spark.shuffle.compress = true\n",
      "spark.executor.id = driver\n",
      "spark.shuffle.spill.compress = true\n",
      "spark.app.name = Experiment 1: KryoSerializer (data_size=small)\n",
      "spark.eventLog.dir = /tmp/spark-events\n",
      "spark.shuffle.file.buffer = 32k\n",
      "spark.serializer = org.apache.spark.serializer.KryoSerializer\n",
      "spark.driver.port = 63422\n",
      "spark.io.compression.codec = lz4\n",
      "spark.rdd.compress = True\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.shuffle.io.preferDirectBufs = true\n",
      "spark.master = local[*]\n",
      "spark.submit.pyFiles = \n",
      "spark.app.startTime = 1710970890982\n",
      "spark.app.submitTime = 1710970750605\n",
      "spark.submit.deployMode = client\n",
      "spark.driver.host = alexandrias-mbp\n",
      "spark.eventLog.compress = true\n",
      "spark.ui.showConsoleProgress = true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 99.09791493415833\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 1: KryoSerializer (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2.1: Decreasing the Shuffle File Buffer\n",
    "- spark.shuffle.file.buffer = 32k -> 16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for SparkConf:\n",
      "spark.eventLog.enabled = true\n",
      "spark.reducer.maxSizeInFlight = 48m\n",
      "spark.serializer = org.apache.spark.serializer.JavaSerializer\n",
      "spark.app.startTime = 1710971067455\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.port = 63531\n",
      "spark.shuffle.compress = true\n",
      "spark.app.id = local-1710971067607\n",
      "spark.executor.id = driver\n",
      "spark.shuffle.spill.compress = true\n",
      "spark.eventLog.dir = /tmp/spark-events\n",
      "spark.shuffle.file.buffer = 16k\n",
      "spark.io.compression.codec = lz4\n",
      "spark.rdd.compress = True\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.shuffle.io.preferDirectBufs = true\n",
      "spark.master = local[*]\n",
      "spark.submit.pyFiles = \n",
      "spark.app.submitTime = 1710970750605\n",
      "spark.app.name = Experiment 2.1: Decrease file_buffer to 16k (data_size=small)\n",
      "spark.submit.deployMode = client\n",
      "spark.driver.host = alexandrias-mbp\n",
      "spark.eventLog.compress = true\n",
      "spark.ui.showConsoleProgress = true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 97.77148795127869\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 2.1: Decrease file_buffer to 16k (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"16k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2.2: Increasing the Shuffle File Buffer\n",
    "- spark.shuffle.file.buffer = 32k -> 64k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for SparkConf:\n",
      "spark.eventLog.enabled = true\n",
      "spark.reducer.maxSizeInFlight = 48m\n",
      "spark.serializer = org.apache.spark.serializer.JavaSerializer\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.app.id = local-1710971167029\n",
      "spark.shuffle.compress = true\n",
      "spark.app.startTime = 1710971166900\n",
      "spark.executor.id = driver\n",
      "spark.shuffle.spill.compress = true\n",
      "spark.eventLog.dir = /tmp/spark-events\n",
      "spark.driver.port = 63636\n",
      "spark.shuffle.file.buffer = 64k\n",
      "spark.io.compression.codec = lz4\n",
      "spark.rdd.compress = True\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.shuffle.io.preferDirectBufs = true\n",
      "spark.master = local[*]\n",
      "spark.submit.pyFiles = \n",
      "spark.app.submitTime = 1710970750605\n",
      "spark.submit.deployMode = client\n",
      "spark.driver.host = alexandrias-mbp\n",
      "spark.eventLog.compress = true\n",
      "spark.app.name = Experiment 2.2: Increase file buffer to 64k (data_size=small)\n",
      "spark.ui.showConsoleProgress = true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 98.58964681625366\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 2.2: Increase file buffer to 64k (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"64k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3.1: Turn off shuffle compress\n",
    "- spark.shuffle.compress = true -> false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for SparkConf:\n",
      "spark.eventLog.enabled = true\n",
      "spark.reducer.maxSizeInFlight = 48m\n",
      "spark.serializer = org.apache.spark.serializer.JavaSerializer\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.executor.id = driver\n",
      "spark.app.startTime = 1710971267109\n",
      "spark.shuffle.spill.compress = true\n",
      "spark.shuffle.file.buffer = 32k\n",
      "spark.eventLog.dir = /tmp/spark-events\n",
      "spark.app.id = local-1710971267183\n",
      "spark.io.compression.codec = lz4\n",
      "spark.rdd.compress = True\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.shuffle.io.preferDirectBufs = true\n",
      "spark.master = local[*]\n",
      "spark.submit.pyFiles = \n",
      "spark.app.submitTime = 1710970750605\n",
      "spark.submit.deployMode = client\n",
      "spark.driver.host = alexandrias-mbp\n",
      "spark.eventLog.compress = true\n",
      "spark.ui.showConsoleProgress = true\n",
      "spark.app.name = Experiment 3.1: No Shuffle Compression (data_size=small)\n",
      "spark.driver.port = 63713\n",
      "spark.shuffle.compress = false\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 102.70968198776245\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 3.1: No Shuffle Compression (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"false\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3.2: Turn off shuffle.spill compress\n",
    "- spark.shuffle.spill.compress = true -> false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for SparkConf:\n",
      "spark.driver.port = 63867\n",
      "spark.eventLog.enabled = true\n",
      "spark.reducer.maxSizeInFlight = 48m\n",
      "spark.serializer = org.apache.spark.serializer.JavaSerializer\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.shuffle.compress = true\n",
      "spark.executor.id = driver\n",
      "spark.shuffle.file.buffer = 32k\n",
      "spark.eventLog.dir = /tmp/spark-events\n",
      "spark.shuffle.spill.compress = false\n",
      "spark.io.compression.codec = lz4\n",
      "spark.rdd.compress = True\n",
      "spark.app.name = Experiment 3.2: No Shuffle Spill Compression (data_size=small)\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.app.startTime = 1710971454185\n",
      "spark.shuffle.io.preferDirectBufs = true\n",
      "spark.submit.pyFiles = \n",
      "spark.app.submitTime = 1710970750605\n",
      "spark.master = local[*]\n",
      "spark.submit.deployMode = client\n",
      "spark.driver.host = alexandrias-mbp\n",
      "spark.eventLog.compress = true\n",
      "spark.ui.showConsoleProgress = true\n",
      "spark.app.id = local-1710971454342\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 98.37648487091064\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 3.2: No Shuffle Spill Compression (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lz4\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"false\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4.1: Change compression codec to: zstd\n",
    "- spark.io.compression.codec = lz4 -> zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for SparkConf:\n",
      "spark.app.name = Experiment 4.1: compression=zstd (data_size=small)\n",
      "spark.eventLog.enabled = true\n",
      "spark.reducer.maxSizeInFlight = 48m\n",
      "spark.serializer = org.apache.spark.serializer.JavaSerializer\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.app.id = local-1710971704216\n",
      "spark.io.compression.codec = zstd\n",
      "spark.shuffle.compress = true\n",
      "spark.executor.id = driver\n",
      "spark.app.startTime = 1710971704152\n",
      "spark.shuffle.spill.compress = true\n",
      "spark.shuffle.file.buffer = 32k\n",
      "spark.eventLog.dir = /tmp/spark-events\n",
      "spark.driver.port = 63979\n",
      "spark.rdd.compress = True\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.shuffle.io.preferDirectBufs = true\n",
      "spark.master = local[*]\n",
      "spark.submit.pyFiles = \n",
      "spark.app.submitTime = 1710970750605\n",
      "spark.submit.deployMode = client\n",
      "spark.driver.host = alexandrias-mbp\n",
      "spark.eventLog.compress = true\n",
      "spark.ui.showConsoleProgress = true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 96.88545989990234\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 4.1: compression=zstd (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"zstd\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4.2: Change compression codec to: snappy\n",
    "- spark.io.compression.codec = lz4 -> snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for SparkConf:\n",
      "spark.eventLog.enabled = true\n",
      "spark.reducer.maxSizeInFlight = 48m\n",
      "spark.serializer = org.apache.spark.serializer.JavaSerializer\n",
      "spark.app.startTime = 1710971877679\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.io.compression.codec = snappy\n",
      "spark.shuffle.compress = true\n",
      "spark.executor.id = driver\n",
      "spark.shuffle.spill.compress = true\n",
      "spark.shuffle.file.buffer = 32k\n",
      "spark.eventLog.dir = /tmp/spark-events\n",
      "spark.rdd.compress = True\n",
      "spark.app.name = Experiment 4.2: compression=snappy (data_size=small)\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.shuffle.io.preferDirectBufs = true\n",
      "spark.master = local[*]\n",
      "spark.submit.pyFiles = \n",
      "spark.app.submitTime = 1710970750605\n",
      "spark.submit.deployMode = client\n",
      "spark.driver.port = 64049\n",
      "spark.driver.host = alexandrias-mbp\n",
      "spark.eventLog.compress = true\n",
      "spark.app.id = local-1710971877834\n",
      "spark.ui.showConsoleProgress = true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 97.54408407211304\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 4.2: compression=snappy (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4.3: Change compression codec to: lzf\n",
    "- spark.io.compression.codec = lz4 -> lzf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spark job for SparkConf:\n",
      "spark.eventLog.enabled = true\n",
      "spark.reducer.maxSizeInFlight = 48m\n",
      "spark.serializer = org.apache.spark.serializer.JavaSerializer\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.io.compression.codec = lzf\n",
      "spark.driver.port = 64128\n",
      "spark.shuffle.compress = true\n",
      "spark.executor.id = driver\n",
      "spark.shuffle.spill.compress = true\n",
      "spark.shuffle.file.buffer = 32k\n",
      "spark.app.startTime = 1710972010763\n",
      "spark.eventLog.dir = /tmp/spark-events\n",
      "spark.app.name = Experiment 4.3: compression=lzf (data_size=small)\n",
      "spark.app.id = local-1710972010852\n",
      "spark.rdd.compress = True\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.shuffle.io.preferDirectBufs = true\n",
      "spark.master = local[*]\n",
      "spark.submit.pyFiles = \n",
      "spark.app.submitTime = 1710970750605\n",
      "spark.submit.deployMode = client\n",
      "spark.driver.host = alexandrias-mbp\n",
      "spark.eventLog.compress = true\n",
      "spark.ui.showConsoleProgress = true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|group_id|avg_subgroup| count|\n",
      "+--------+------------+------+\n",
      "|       0|       450.0|100000|\n",
      "|      10|       451.0|100000|\n",
      "|      20|       452.0|100000|\n",
      "|      30|       453.0|100000|\n",
      "|      40|       454.0|100000|\n",
      "|      50|       455.0|100000|\n",
      "|      60|       456.0|100000|\n",
      "|      70|       457.0|100000|\n",
      "|      80|       458.0|100000|\n",
      "|      90|       459.0|100000|\n",
      "|     100|       460.0|100000|\n",
      "|     110|       461.0|100000|\n",
      "|     120|       462.0|100000|\n",
      "|     130|       463.0|100000|\n",
      "|     140|       464.0|100000|\n",
      "|     150|       465.0|100000|\n",
      "|     160|       466.0|100000|\n",
      "|     170|       467.0|100000|\n",
      "|     180|       468.0|100000|\n",
      "|     190|       469.0|100000|\n",
      "+--------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time (secs): 96.0404200553894\n"
     ]
    }
   ],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Experiment 4.3: compression=lzf (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"lzf\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.JavaSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"32k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Optimization\n",
    "\n",
    "TODO: Figure out the final optimization\n",
    "\n",
    "- spark.serializer = JavaSerializer -> KryoSerializer\n",
    "\n",
    "- spark.shuffle.compress = true -> false\n",
    "\n",
    "- spark.shuffle.spill.compress = true -> false\n",
    "\n",
    "- spark.shuffle.file.buffer = 32k -> 64k\n",
    "\n",
    "- spark.io.compression.codec = lz4 -> zstd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the job with default config\n",
    "data_size_label = data_size_labels.get(DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE)\n",
    "app_name = f'Final Optimization (data_size={data_size_label})'\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(app_name) \\\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"false\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"zstd\") \\\n",
    "    .config(\"spark.shuffle.io.preferDirectBufs\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KyroSerializer\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"64k\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.compress\", \"true\")\\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\\\n",
    "    .getOrCreate()\n",
    "run_large_spark_job(spark=spark, data_size=DEFAULT_DATA_SIZE, app_name=app_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (METCS777 Term Paper)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
